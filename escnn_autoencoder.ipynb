{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YauIZ8KLA7kY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julballa/LatentLattice/blob/main/escnn_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "hrjmbVb-MSMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set current directory\n",
        "# this should be the Google Drive folder where your file(s) are located\n",
        "%cd /content/drive/MyDrive/lattices\n",
        "\n",
        "## verify current directory\n",
        "!ls /content/drive/MyDrive/lattices\n",
        "\n",
        "# choose where you want your project files to be saved\n",
        "project_folder = \"/content/drive/MyDrive/lattices\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6aKrI1ZvroP",
        "outputId": "0e273b0b-f709-416f-ecb0-eaed328925d0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/lattices\n",
            "12x12_train.pt\t12x12_val.pt  ten_square_test.pt  ten_square_train.pt  ten_square_val.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AMLab-Amsterdam/lie_learn escnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGO5Cszxeyn7",
        "outputId": "d02c5f24-0ea1-44fc-b2e8-b8fe27cad0fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/AMLab-Amsterdam/lie_learn\n",
            "  Cloning https://github.com/AMLab-Amsterdam/lie_learn to /tmp/pip-req-build-hppoyc09\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AMLab-Amsterdam/lie_learn /tmp/pip-req-build-hppoyc09\n",
            "  Resolved https://github.com/AMLab-Amsterdam/lie_learn to commit 1ccc2106e402d517a29de5438c9367c959e67338\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: escnn in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (1.9.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from escnn) (2.2.1+cu121)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from escnn) (1.4.0)\n",
            "Requirement already satisfied: pymanopt in /usr/local/lib/python3.10/dist-packages (from escnn) (2.2.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from escnn) (1.6.2)\n",
            "Requirement already satisfied: py3nj in /usr/local/lib/python3.10/dist-packages (from escnn) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->escnn) (12.4.127)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->escnn) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->escnn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->escnn) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "uSx8pIYWAN2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from escnn.nn import R3Conv, R3ConvTransposed, LeakyReLU, GroupPooling, GeometricTensor, FieldType\n",
        "from escnn.gspaces.r3 import flipRot3dOnR3\n",
        "# 2D data imports\n",
        "from escnn.nn import R2Conv, R2ConvTransposed\n",
        "from escnn.gspaces import rot2dOnR2\n",
        "\n",
        "# Encoder -- 2 layer network\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R2Conv(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R2Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act2 = LeakyReLU(out_type)\n",
        "        self.pool = GroupPooling(out_type)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R2ConvTransposed(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type, latent_type):\n",
        "        super().__init__()\n",
        "        self.in_type = in_type\n",
        "        self.out_type = out_type\n",
        "        self.latent_type = latent_type\n",
        "\n",
        "        self.encoder = Encoder(in_type, latent_type)\n",
        "        self.decoder = Decoder(latent_type, out_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = GeometricTensor(x, self.in_type)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat.tensor"
      ],
      "metadata": {
        "id": "98hB2QoEr7os"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from escnn.nn import R2Conv, R2ConvTransposed, LeakyReLU, GroupPooling, GeometricTensor, FieldType, IIDBatchNorm2d\n",
        "from escnn.gspaces import rot2dOnR2\n",
        "\n",
        "# Encoder -- 2 layer with Batch Norm\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R2Conv(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.bn1 = IIDBatchNorm2d(out_type)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R2Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "        self.bn2 = IIDBatchNorm2d(out_type)\n",
        "        self.act2 = LeakyReLU(out_type)\n",
        "        self.pool = GroupPooling(out_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R2ConvTransposed(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.bn1 = IIDBatchNorm2d(out_type)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "        self.bn2 = IIDBatchNorm2d(out_type)\n",
        "        self.act2 = LeakyReLU(out_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type, latent_type):\n",
        "        super().__init__()\n",
        "        self.in_type = in_type\n",
        "        self.out_type = out_type\n",
        "        self.latent_type = latent_type\n",
        "\n",
        "        self.encoder = Encoder(in_type, latent_type)\n",
        "        self.decoder = Decoder(latent_type, out_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = GeometricTensor(x, self.in_type)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat.tensor\n"
      ],
      "metadata": {
        "id": "n7ZQNF9ntzGP"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# from escnn.nn import R2Conv, R2ConvTransposed, LeakyReLU, GroupPooling, GeometricTensor, FieldType\n",
        "# from escnn.gspaces import rot2dOnR2\n",
        "\n",
        "# # Encoder -- 3 layer network\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = R2Conv(in_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.act1 = LeakyReLU(out_type)\n",
        "#         self.conv2 = R2Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.act2 = LeakyReLU(out_type)\n",
        "#         self.conv3 = R2Conv(out_type, out_type, kernel_size=3, stride=1)  # Additional layer\n",
        "#         self.act3 = LeakyReLU(out_type)\n",
        "#         self.pool = GroupPooling(out_type)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.act1(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.act2(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.act3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# # Decoder\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = R2ConvTransposed(in_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.act1 = LeakyReLU(out_type)\n",
        "#         self.conv2 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.act2 = LeakyReLU(out_type)  # Additional activation\n",
        "#         self.conv3 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)  # Additional layer\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         self.act1 = LeakyReLU(out_type)\n",
        "#         x = self.conv2(x)\n",
        "#         self.act2 = LeakyReLU(out_type)\n",
        "#         x = self.conv3(x) # Additional layer processing\n",
        "#         return x\n",
        "\n",
        "# class AutoEncoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type, latent_type):\n",
        "#         super().__init__()\n",
        "#         self.in_type = in_type\n",
        "#         self.out_type = out_type\n",
        "#         self.latent_type = latent_type\n",
        "\n",
        "#         self.encoder = Encoder(in_type, latent_type)\n",
        "#         self.decoder = Decoder(latent_type, out_type)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = GeometricTensor(x, self.in_type)\n",
        "#         z = self.encoder(x)\n",
        "#         x_hat = self.decoder(z)\n",
        "#         return x_hat.tensor\n"
      ],
      "metadata": {
        "id": "EhU9W5VAlPl5"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# from escnn.nn import R2Conv, R2ConvTransposed, LeakyReLU, GroupPooling, GeometricTensor, FieldType, IIDBatchNorm2d\n",
        "# from escnn.gspaces import rot2dOnR2\n",
        "\n",
        "# # Encoder -- 3 layer network with BatchNorm\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = R2Conv(in_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn1 = IIDBatchNorm2d(out_type)\n",
        "#         self.act1 = LeakyReLU(out_type)\n",
        "#         self.conv2 = R2Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn2 = IIDBatchNorm2d(out_type)\n",
        "#         self.act2 = LeakyReLU(out_type)\n",
        "#         self.conv3 = R2Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn3 = IIDBatchNorm2d(out_type)\n",
        "#         self.act3 = LeakyReLU(out_type)\n",
        "#         self.pool = GroupPooling(out_type)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = self.act1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.bn2(x)\n",
        "#         x = self.act2(x)\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.bn3(x)\n",
        "#         x = self.act3(x)\n",
        "#         x = self.pool(x)\n",
        "#         return x\n",
        "\n",
        "# # Decoder\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = R2ConvTransposed(in_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn1 = IIDBatchNorm2d(out_type)\n",
        "#         self.act1 = LeakyReLU(out_type)\n",
        "#         self.conv2 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn2 = IIDBatchNorm2d(out_type)\n",
        "#         self.act2 = LeakyReLU(out_type)\n",
        "#         self.conv3 = R2ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "#         self.bn3 = IIDBatchNorm2d(out_type)\n",
        "#         self.act3 = LeakyReLU(out_type)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = self.act1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.bn2(x)\n",
        "#         x = self.act2(x)\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.bn3(x)\n",
        "#         x = self.act3(x)\n",
        "#         return x\n",
        "\n",
        "# class AutoEncoder(nn.Module):\n",
        "#     def __init__(self, in_type, out_type, latent_type):\n",
        "#         super().__init__()\n",
        "#         self.in_type = in_type\n",
        "#         self.out_type = out_type\n",
        "#         self.latent_type = latent_type\n",
        "\n",
        "#         self.encoder = Encoder(in_type, latent_type)\n",
        "#         self.decoder = Decoder(latent_type, out_type)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = GeometricTensor(x, self.in_type)\n",
        "#         z = self.encoder(x)\n",
        "#         x_hat = self.decoder(z)\n",
        "#         return x_hat.tensor\n"
      ],
      "metadata": {
        "id": "vPbKp2fjrLZo"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "YauIZ8KLA7kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMFGRFWv_Q3p",
        "outputId": "1fd8a631-a60f-486e-9c0f-d43e161eaa19"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.9.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class LatticeDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        \"\"\"\n",
        "        data_list is a list of dictionaries, each containing keys 'x', 'edge_index', and 'coords'.\n",
        "        'x' holds the features of the nodes, which are one-hot encoded.\n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the data point\n",
        "        data = self.data_list[idx]\n",
        "\n",
        "        # The node features are one-hot encoded and should be reshaped into a 12x12 grid\n",
        "        node_features = data.x.view(1, 12, 12).float()  # Assuming the features are flat [144, 1]\n",
        "\n",
        "        # Since this is an autoencoder, the input is the target\n",
        "        return node_features, node_features\n",
        "\n",
        "train_data = torch.load('12x12_train.pt')\n",
        "val_data = torch.load('12x12_val.pt')\n",
        "# test_data = torch.load('ten_square_test.pt')\n",
        "\n",
        "train_dataset = LatticeDataset(train_data)\n",
        "val_dataset = LatticeDataset(val_data)\n",
        "# test_dataset = LatticeDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "eZr90t60Hgg1"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "kQPzSvDt2z2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "# Init model\n",
        "in_type = FieldType(rot2dOnR2(), [rot2dOnR2().trivial_repr])\n",
        "latent_type = FieldType(rot2dOnR2(), [rot2dOnR2().trivial_repr])\n",
        "out_type = FieldType(rot2dOnR2(), [rot2dOnR2().trivial_repr]*2)\n",
        "\n",
        "model = AutoEncoder(in_type, out_type, latent_type)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05)  # Optimizer\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
        "\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device).squeeze().long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device).squeeze().long()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "0eZMYuh2sQ-C",
        "outputId": "21fe4457-1965-41ce-ca07-c3651f256252"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cpu\n",
            "Epoch 1/500, Train Loss: 0.5114, Validation Loss: 1.5878\n",
            "Epoch 2/500, Train Loss: 0.4323, Validation Loss: 0.4230\n",
            "Epoch 3/500, Train Loss: 0.4016, Validation Loss: 0.6046\n",
            "Epoch 4/500, Train Loss: 0.3862, Validation Loss: 0.4734\n",
            "Epoch 5/500, Train Loss: 0.3822, Validation Loss: 0.3928\n",
            "Epoch 6/500, Train Loss: 0.3800, Validation Loss: 0.3958\n",
            "Epoch 7/500, Train Loss: 0.3788, Validation Loss: 0.3809\n",
            "Epoch 8/500, Train Loss: 0.3774, Validation Loss: 0.3860\n",
            "Epoch 9/500, Train Loss: 0.3763, Validation Loss: 0.3799\n",
            "Epoch 10/500, Train Loss: 0.3755, Validation Loss: 0.3911\n",
            "Epoch 11/500, Train Loss: 0.3748, Validation Loss: 0.3759\n",
            "Epoch 12/500, Train Loss: 0.3740, Validation Loss: 0.3739\n",
            "Epoch 13/500, Train Loss: 0.3734, Validation Loss: 0.4031\n",
            "Epoch 14/500, Train Loss: 0.3728, Validation Loss: 0.3780\n",
            "Epoch 15/500, Train Loss: 0.3723, Validation Loss: 0.3717\n",
            "Epoch 16/500, Train Loss: 0.3717, Validation Loss: 0.3793\n",
            "Epoch 17/500, Train Loss: 0.3714, Validation Loss: 0.3764\n",
            "Epoch 18/500, Train Loss: 0.3711, Validation Loss: 0.3714\n",
            "Epoch 19/500, Train Loss: 0.3706, Validation Loss: 0.3703\n",
            "Epoch 20/500, Train Loss: 0.3704, Validation Loss: 0.3697\n",
            "Epoch 21/500, Train Loss: 0.3701, Validation Loss: 0.3699\n",
            "Epoch 22/500, Train Loss: 0.3699, Validation Loss: 0.3690\n",
            "Epoch 23/500, Train Loss: 0.3696, Validation Loss: 0.3736\n",
            "Epoch 24/500, Train Loss: 0.3694, Validation Loss: 0.4136\n",
            "Epoch 25/500, Train Loss: 0.3693, Validation Loss: 0.3686\n",
            "Epoch 26/500, Train Loss: 0.3690, Validation Loss: 0.3744\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-b83f2396562e>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ucNK7XDtw4d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6wn3dJmAik2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}