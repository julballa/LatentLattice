{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "hrjmbVb-MSMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set current directory\n",
        "# this should be the Google Drive folder where your file(s) are located\n",
        "%cd /content/drive/MyDrive/lattices\n",
        "\n",
        "## verify current directory\n",
        "!ls /content/drive/MyDrive/lattices\n",
        "\n",
        "# choose where you want your project files to be saved\n",
        "project_folder = \"/content/drive/MyDrive/lattices\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6aKrI1ZvroP",
        "outputId": "d8260e16-01e2-4941-c108-536da4a67a31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/lattices\n",
            "20240504_2031  20240505_0042  20240505_0120  20240505_0148  20240505_0244  20240505_0249\n",
            "20240505_0005  20240505_0045  20240505_0135  20240505_0214  20240505_0245  data\n",
            "20240505_0013  20240505_0049  20240505_0136  20240505_0217  20240505_0246  LatentLattice\n",
            "20240505_0040  20240505_0114  20240505_0137  20240505_0218  20240505_0247\n",
            "20240505_0041  20240505_0119  20240505_0140  20240505_0233  20240505_0248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AMLab-Amsterdam/lie_learn escnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGO5Cszxeyn7",
        "outputId": "a608e6f6-edff-40a4-cfa3-77c34f690f7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/AMLab-Amsterdam/lie_learn\n",
            "  Cloning https://github.com/AMLab-Amsterdam/lie_learn to /tmp/pip-req-build-19rcal8p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AMLab-Amsterdam/lie_learn /tmp/pip-req-build-19rcal8p\n",
            "  Resolved https://github.com/AMLab-Amsterdam/lie_learn to commit 1ccc2106e402d517a29de5438c9367c959e67338\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting escnn\n",
            "  Downloading escnn-1.0.11-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.9/373.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lie_learn==0.0.1.post1) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from escnn) (2.2.1+cu121)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from escnn) (1.4.0)\n",
            "Collecting pymanopt (from escnn)\n",
            "  Downloading pymanopt-2.2.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from escnn) (1.6.2)\n",
            "Collecting py3nj (from escnn)\n",
            "  Downloading py3nj-0.2.1.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->escnn)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->escnn) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->escnn)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->escnn) (0.18.3)\n",
            "Collecting scipy (from lie_learn==0.0.1.post1)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lie_learn==0.0.1.post1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->escnn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->escnn) (1.3.0)\n",
            "Building wheels for collected packages: lie_learn, py3nj\n",
            "  Building wheel for lie_learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lie_learn: filename=lie_learn-0.0.1.post1-cp310-cp310-linux_x86_64.whl size=16176489 sha256=42e442fef921f92d1f3dcdbb808c5ef7dacfd46087270dc857f224136a5324bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bbapa5_c/wheels/3f/33/85/b8725ee77011bc42d77e4e35aeca2088482c3094f5c0a650a6\n",
            "  Building wheel for py3nj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py3nj: filename=py3nj-0.2.1-cp310-cp310-linux_x86_64.whl size=44135 sha256=a3193ca21b3579cd82fdf88c4559a3b4a96d2d1418b4e214f30a704caec950f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/e9/70/30a34ed6dbc8b54ce93f25c091be4cf7a24319e27d953a882b\n",
            "Successfully built lie_learn py3nj\n",
            "Installing collected packages: scipy, py3nj, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pymanopt, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lie_learn, nvidia-cusolver-cu12, escnn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "Successfully installed escnn-1.0.11 lie_learn-0.0.1.post1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 py3nj-0.2.1 pymanopt-2.2.0 scipy-1.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "uSx8pIYWAN2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from escnn.nn import R3Conv, R3ConvTransposed, LeakyReLU, GroupPooling, GeometricTensor, FieldType\n",
        "from escnn.gspaces.r3 import flipRot3dOnR3\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R3Conv(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R3Conv(out_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act2 = LeakyReLU(out_type)\n",
        "        self.pool = GroupPooling(out_type)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type):\n",
        "        super().__init__()\n",
        "        self.conv1 = R3ConvTransposed(in_type, out_type, kernel_size=3, stride=1)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        self.conv2 = R3ConvTransposed(out_type, out_type, kernel_size=3, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        self.act1 = LeakyReLU(out_type)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_type, out_type, latent_type):\n",
        "        super().__init__()\n",
        "        self.in_type = in_type\n",
        "        self.out_type = out_type\n",
        "        self.latent_type = latent_type\n",
        "\n",
        "        self.encoder = Encoder(in_type, latent_type)\n",
        "        self.decoder = Decoder(latent_type, out_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = GeometricTensor(x, self.in_type)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat.tensor"
      ],
      "metadata": {
        "id": "98hB2QoEr7os"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "YauIZ8KLA7kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LatticeDataset(Dataset):\n",
        "    def __init__(self, shape, length, num_node_types=2):\n",
        "        self.length = length\n",
        "        self.data = [torch.randint(0, num_node_types, shape).float() for _ in range(length)]  # 1 channel, size x size lattices\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.data[idx]\n",
        "\n",
        "shape = (1,8,8,8)\n",
        "train_dataset = LatticeDataset(shape=shape, length=1000, num_node_types=2)\n",
        "val_dataset = LatticeDataset(shape=shape, length=200, num_node_types=2)\n",
        "test_dataset = LatticeDataset(shape=shape, length=200, num_node_types=2)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "wqaq1ABfA8fG"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "kQPzSvDt2z2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "# Init model\n",
        "in_type = FieldType(flipRot3dOnR3(), [flipRot3dOnR3().trivial_repr])\n",
        "latent_type = FieldType(flipRot3dOnR3(), [flipRot3dOnR3().trivial_repr])\n",
        "out_type = FieldType(flipRot3dOnR3(), [flipRot3dOnR3().trivial_repr]*2)\n",
        "\n",
        "model = AutoEncoder(in_type, out_type, latent_type)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Optimizer\n",
        "\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device).squeeze().long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device).squeeze().long()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0eZMYuh2sQ-C",
        "outputId": "01483b7a-a86b-44d5-f47b-0e54a9b58561"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cpu\n",
            "Epoch 1/500, Train Loss: 0.6931, Validation Loss: 0.6931\n",
            "Epoch 2/500, Train Loss: 0.6930, Validation Loss: 0.6929\n",
            "Epoch 3/500, Train Loss: 0.6929, Validation Loss: 0.6926\n",
            "Epoch 4/500, Train Loss: 0.6921, Validation Loss: 0.6920\n",
            "Epoch 5/500, Train Loss: 0.6900, Validation Loss: 0.6890\n",
            "Epoch 6/500, Train Loss: 0.6883, Validation Loss: 0.6866\n",
            "Epoch 7/500, Train Loss: 0.6861, Validation Loss: 0.6849\n",
            "Epoch 8/500, Train Loss: 0.6851, Validation Loss: 0.6840\n",
            "Epoch 9/500, Train Loss: 0.6841, Validation Loss: 0.6823\n",
            "Epoch 10/500, Train Loss: 0.6820, Validation Loss: 0.6791\n",
            "Epoch 11/500, Train Loss: 0.6767, Validation Loss: 0.6701\n",
            "Epoch 12/500, Train Loss: 0.6621, Validation Loss: 0.6484\n",
            "Epoch 13/500, Train Loss: 0.6394, Validation Loss: 0.6313\n",
            "Epoch 14/500, Train Loss: 0.6296, Validation Loss: 0.6279\n",
            "Epoch 15/500, Train Loss: 0.6276, Validation Loss: 0.6269\n",
            "Epoch 16/500, Train Loss: 0.6267, Validation Loss: 0.6262\n",
            "Epoch 17/500, Train Loss: 0.6261, Validation Loss: 0.6257\n",
            "Epoch 18/500, Train Loss: 0.6255, Validation Loss: 0.6253\n",
            "Epoch 19/500, Train Loss: 0.6251, Validation Loss: 0.6248\n",
            "Epoch 20/500, Train Loss: 0.6244, Validation Loss: 0.6240\n",
            "Epoch 21/500, Train Loss: 0.6251, Validation Loss: 0.6253\n",
            "Epoch 22/500, Train Loss: 0.6236, Validation Loss: 0.6231\n",
            "Epoch 23/500, Train Loss: 0.6225, Validation Loss: 0.6222\n",
            "Epoch 24/500, Train Loss: 0.6216, Validation Loss: 0.6213\n",
            "Epoch 25/500, Train Loss: 0.6207, Validation Loss: 0.6204\n",
            "Epoch 26/500, Train Loss: 0.6199, Validation Loss: 0.6198\n",
            "Epoch 27/500, Train Loss: 0.6191, Validation Loss: 0.6191\n",
            "Epoch 28/500, Train Loss: 0.6186, Validation Loss: 0.6187\n",
            "Epoch 29/500, Train Loss: 0.6182, Validation Loss: 0.6183\n",
            "Epoch 30/500, Train Loss: 0.6179, Validation Loss: 0.6181\n",
            "Epoch 31/500, Train Loss: 0.6176, Validation Loss: 0.6177\n",
            "Epoch 32/500, Train Loss: 0.6173, Validation Loss: 0.6176\n",
            "Epoch 33/500, Train Loss: 0.6170, Validation Loss: 0.6173\n",
            "Epoch 34/500, Train Loss: 0.6169, Validation Loss: 0.6171\n",
            "Epoch 35/500, Train Loss: 0.6166, Validation Loss: 0.6168\n",
            "Epoch 36/500, Train Loss: 0.6163, Validation Loss: 0.6165\n",
            "Epoch 37/500, Train Loss: 0.6161, Validation Loss: 0.6163\n",
            "Epoch 38/500, Train Loss: 0.6158, Validation Loss: 0.6162\n",
            "Epoch 39/500, Train Loss: 0.6157, Validation Loss: 0.6158\n",
            "Epoch 40/500, Train Loss: 0.6152, Validation Loss: 0.6155\n",
            "Epoch 41/500, Train Loss: 0.6149, Validation Loss: 0.6152\n",
            "Epoch 42/500, Train Loss: 0.6146, Validation Loss: 0.6147\n",
            "Epoch 43/500, Train Loss: 0.6143, Validation Loss: 0.6144\n",
            "Epoch 44/500, Train Loss: 0.6138, Validation Loss: 0.6140\n",
            "Epoch 45/500, Train Loss: 0.6133, Validation Loss: 0.6135\n",
            "Epoch 46/500, Train Loss: 0.6129, Validation Loss: 0.6130\n",
            "Epoch 47/500, Train Loss: 0.6123, Validation Loss: 0.6124\n",
            "Epoch 48/500, Train Loss: 0.6119, Validation Loss: 0.6121\n",
            "Epoch 49/500, Train Loss: 0.6113, Validation Loss: 0.6114\n",
            "Epoch 50/500, Train Loss: 0.6107, Validation Loss: 0.6107\n",
            "Epoch 51/500, Train Loss: 0.6102, Validation Loss: 0.6118\n",
            "Epoch 52/500, Train Loss: 0.6099, Validation Loss: 0.6096\n",
            "Epoch 53/500, Train Loss: 0.6091, Validation Loss: 0.6092\n",
            "Epoch 54/500, Train Loss: 0.6085, Validation Loss: 0.6090\n",
            "Epoch 55/500, Train Loss: 0.6081, Validation Loss: 0.6082\n",
            "Epoch 56/500, Train Loss: 0.6077, Validation Loss: 0.6078\n",
            "Epoch 57/500, Train Loss: 0.6073, Validation Loss: 0.6075\n",
            "Epoch 58/500, Train Loss: 0.6069, Validation Loss: 0.6073\n",
            "Epoch 59/500, Train Loss: 0.6067, Validation Loss: 0.6070\n",
            "Epoch 60/500, Train Loss: 0.6066, Validation Loss: 0.6070\n",
            "Epoch 61/500, Train Loss: 0.6063, Validation Loss: 0.6063\n",
            "Epoch 62/500, Train Loss: 0.6060, Validation Loss: 0.6061\n",
            "Epoch 63/500, Train Loss: 0.6060, Validation Loss: 0.6065\n",
            "Epoch 64/500, Train Loss: 0.6058, Validation Loss: 0.6059\n",
            "Epoch 65/500, Train Loss: 0.6057, Validation Loss: 0.6059\n",
            "Epoch 66/500, Train Loss: 0.6057, Validation Loss: 0.6058\n",
            "Epoch 67/500, Train Loss: 0.6054, Validation Loss: 0.6057\n",
            "Epoch 68/500, Train Loss: 0.6054, Validation Loss: 0.6059\n",
            "Epoch 69/500, Train Loss: 0.6053, Validation Loss: 0.6055\n",
            "Epoch 70/500, Train Loss: 0.6054, Validation Loss: 0.6057\n",
            "Epoch 71/500, Train Loss: 0.6053, Validation Loss: 0.6054\n",
            "Epoch 72/500, Train Loss: 0.6052, Validation Loss: 0.6055\n",
            "Epoch 73/500, Train Loss: 0.6054, Validation Loss: 0.6054\n",
            "Epoch 74/500, Train Loss: 0.6053, Validation Loss: 0.6057\n",
            "Epoch 75/500, Train Loss: 0.6054, Validation Loss: 0.6055\n",
            "Epoch 76/500, Train Loss: 0.6050, Validation Loss: 0.6056\n",
            "Epoch 77/500, Train Loss: 0.6050, Validation Loss: 0.6054\n",
            "Epoch 78/500, Train Loss: 0.6051, Validation Loss: 0.6053\n",
            "Epoch 79/500, Train Loss: 0.6050, Validation Loss: 0.6054\n",
            "Epoch 80/500, Train Loss: 0.6050, Validation Loss: 0.6052\n",
            "Epoch 81/500, Train Loss: 0.6050, Validation Loss: 0.6052\n",
            "Epoch 82/500, Train Loss: 0.6050, Validation Loss: 0.6055\n",
            "Epoch 83/500, Train Loss: 0.6049, Validation Loss: 0.6052\n",
            "Epoch 84/500, Train Loss: 0.6051, Validation Loss: 0.6052\n",
            "Epoch 85/500, Train Loss: 0.6050, Validation Loss: 0.6054\n",
            "Epoch 86/500, Train Loss: 0.6050, Validation Loss: 0.6052\n",
            "Epoch 87/500, Train Loss: 0.6050, Validation Loss: 0.6051\n",
            "Epoch 88/500, Train Loss: 0.6051, Validation Loss: 0.6064\n",
            "Epoch 89/500, Train Loss: 0.6050, Validation Loss: 0.6053\n",
            "Epoch 90/500, Train Loss: 0.6050, Validation Loss: 0.6051\n",
            "Epoch 91/500, Train Loss: 0.6050, Validation Loss: 0.6051\n",
            "Epoch 92/500, Train Loss: 0.6049, Validation Loss: 0.6054\n",
            "Epoch 93/500, Train Loss: 0.6050, Validation Loss: 0.6053\n",
            "Epoch 94/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 95/500, Train Loss: 0.6049, Validation Loss: 0.6051\n",
            "Epoch 96/500, Train Loss: 0.6049, Validation Loss: 0.6050\n",
            "Epoch 97/500, Train Loss: 0.6049, Validation Loss: 0.6053\n",
            "Epoch 98/500, Train Loss: 0.6049, Validation Loss: 0.6052\n",
            "Epoch 99/500, Train Loss: 0.6048, Validation Loss: 0.6052\n",
            "Epoch 100/500, Train Loss: 0.6048, Validation Loss: 0.6050\n",
            "Epoch 101/500, Train Loss: 0.6048, Validation Loss: 0.6054\n",
            "Epoch 102/500, Train Loss: 0.6049, Validation Loss: 0.6054\n",
            "Epoch 103/500, Train Loss: 0.6049, Validation Loss: 0.6050\n",
            "Epoch 104/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 105/500, Train Loss: 0.6049, Validation Loss: 0.6054\n",
            "Epoch 106/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 107/500, Train Loss: 0.6050, Validation Loss: 0.6052\n",
            "Epoch 108/500, Train Loss: 0.6049, Validation Loss: 0.6051\n",
            "Epoch 109/500, Train Loss: 0.6049, Validation Loss: 0.6053\n",
            "Epoch 110/500, Train Loss: 0.6048, Validation Loss: 0.6052\n",
            "Epoch 111/500, Train Loss: 0.6049, Validation Loss: 0.6052\n",
            "Epoch 112/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 113/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 114/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 115/500, Train Loss: 0.6049, Validation Loss: 0.6052\n",
            "Epoch 116/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 117/500, Train Loss: 0.6049, Validation Loss: 0.6054\n",
            "Epoch 118/500, Train Loss: 0.6048, Validation Loss: 0.6054\n",
            "Epoch 119/500, Train Loss: 0.6048, Validation Loss: 0.6052\n",
            "Epoch 120/500, Train Loss: 0.6049, Validation Loss: 0.6051\n",
            "Epoch 121/500, Train Loss: 0.6050, Validation Loss: 0.6050\n",
            "Epoch 122/500, Train Loss: 0.6048, Validation Loss: 0.6056\n",
            "Epoch 123/500, Train Loss: 0.6049, Validation Loss: 0.6051\n",
            "Epoch 124/500, Train Loss: 0.6048, Validation Loss: 0.6051\n",
            "Epoch 125/500, Train Loss: 0.6048, Validation Loss: 0.6052\n",
            "Epoch 126/500, Train Loss: 0.6049, Validation Loss: 0.6050\n",
            "Epoch 127/500, Train Loss: 0.6048, Validation Loss: 0.6050\n",
            "Epoch 128/500, Train Loss: 0.6049, Validation Loss: 0.6051\n",
            "Epoch 129/500, Train Loss: 0.6049, Validation Loss: 0.6050\n",
            "Epoch 130/500, Train Loss: 0.6047, Validation Loss: 0.6051\n",
            "Epoch 131/500, Train Loss: 0.6049, Validation Loss: 0.6057\n",
            "Epoch 132/500, Train Loss: 0.6048, Validation Loss: 0.6049\n",
            "Epoch 133/500, Train Loss: 0.6049, Validation Loss: 0.6051\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-d577de125f7c>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/decorators.py\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minnermost_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/package/package_importer.py\u001b[0m in \u001b[0;36m_patched_getfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_getfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mistraceback\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    353\u001b[0m             isinstance(object, collections.abc.Awaitable))\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \"\"\"Return true if the object is a traceback.\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}